'''
Step1. Update Q-function
Step2. Update policy
'''
import csv
import time
import copy
from Environments.CaE import Survive_small
from Functions.Core import Transaction, Trajectory, JointPolicy, QTable
import numpy as np
from Environments.GatherEnv.Lever import Lever
# from Environments.GatherEnv.onedim_modifyreward import OneDimGather
import arguments
import scipy.optimize as opt
import numba as nb
from numba import jit
from Functions.OptFuncs_LP import *



# def onehot_action(actions, max_acts):
#     ## Suppose max action is 3, then transfer [0,2,1] to [1,0,0,0,0,1,0,1,0]
#     result = []
#     for a in actions:
#         act = [0 for _ in range(max_acts)]
#         act[a] = 1
#         result += act
#     return result


def pick_action(state, policy):
    state = int(state)
    act_probs = policy.policy[state]
    act = np.random.choice(a=np.arange(len(act_probs)), size=1, p=act_probs)
    return int(act[0])


def generate_history(hor, policy, env, num_game=100):
    history = Trajectory(max_len=num_game *
                         hor)  # History generated by a specific policy
    for _ in range(num_game):  # Each game
        obs = env.reset()
        for t in range(hor):  # Each step
            state_id = env.vec_to_ind(obs)
            action_id = pick_action(state_id, policy)
            action = env.ind_to_act(action_id)
            new_obs, reward, done, info = env.step(action)
            if done:  # Terminate state
                new_obs = None
            trans = Transaction(obs, action, reward, new_obs)
            history.add(trans)
            obs = new_obs
            if done:
                break
    return history


def main(obj_name, algo_name, env_name):
    if env_name == 'Lever':
        env_used = Lever
    elif env_name == 'SurviveSmall':
        env_used = Survive_small
    arglist = arguments.parse_args()
    #algo_name = 'q-based'
    game_name = 'OneDimGather'
    gamma = arglist.gamma
    horizon = arglist.horizon

    csv_header = ['SolutionConcept', 'Method', 'Epoch', 'Iteration', 'Risk', 'ExpectedReward', 'BFViolated', 'RegretViolated']
    file_name = "./Result/" + algo_name + env_name + obj_name + str(
        time.localtime().tm_mon) + '-' + str(
            time.localtime().tm_mday) + '-' + str(
                time.localtime().tm_hour) + '-' + str(
                    time.localtime().tm_min) + '-' + str(
                        time.localtime().tm_sec) + ".csv"
    f = open(file_name, 'a', newline='')
    writer = csv.writer(f)
    writer.writerow(csv_header)
    f.close()
    # Some global variables
    if algo_name == 'ModRew':
        foo_env = env_used(mod_rew=True)
    else:
        foo_env = env_used()
    state_n = foo_env.possible_states
    jact_n = foo_env.joint_act_nums
    gamma_v = np.sum(np.array([np.power(0.99,i) for i in range(horizon)]))
    ori_idx = [foo_env.get_ori_idx(i) for i in range(foo_env.num_agents)]
    alter_idx = [foo_env.get_alter_idx(i) for i in range(foo_env.num_agents)]
    for epc in range(int(arglist.num_runs)):  # Each experiment
        
        if algo_name == 'ModRew':
            env = env_used(mod_rew=True)
            policy = JointPolicy(env=env_used(mod_rew=True))
        else:
            env = env_used()
            policy = JointPolicy(env=env_used())
        env.reset()
        qt_tables = [QTable(state_num=env.possible_states,
                                    action_num=env.action_space,agent_num=
                                    env.num_agents) for _ in range(env.num_agents)]

        for iter in range(int(arglist.max_iter)):
            f = open(file_name, 'a', newline='')
            writer = csv.writer(f)
            print("iteration %d" % (iter + 1))
            ## Use current policy generate history
            history = generate_history(horizon, policy=policy, env=env)
            print("##### History generated #####")
            ## Train Q-net according to the history
            print("### Updating Q-table for game %d ###" % (iter + 1))
            last_loss = None
            for n in range(int(arglist.q_net_max_iter)):
                old_tables = [copy.deepcopy(qt_tables[i]) for i in range(env.num_agents)]
                samples = history.get_sample()
                for sample in samples:
                    sample_a = env.jointact_to_ind(sample.action)
                    sample_s = env.vec_to_ind(sample.state)
                    if sample.state_next is not None:  # Non-Terminate state
                        sample_a_new = env.jointact_to_ind(sample.act_next)
                        sample_s_new = env.vec_to_ind(sample.state_next)
                        for i in range(env.num_agents):
                            qt_tables[i].update_q_sampling(sample.reward[i],
                                          s=sample_s,
                                          a=sample_a,
                                          s_new = sample_s_new,
                                          a_new = sample_a_new)
                    else:
                        for i in range(env.num_agents):
                            qt_tables[i].update_q_done(sample.reward[i],
                                                s=sample_s,
                                                a=sample_a)
                dif_qs = [abs(np.sum(qt_tables[i].qt - old_tables[i].qt)) for i in range(env.num_agents)]
                if sum(dif_qs) < 0.05:
                    print("Terminate at iter %d" % (n + 1))
                    break
            print("### Solve the CE ###")
            """
            Objective is minimizing sum among a of density of s*,a
            Constraint1 is occumeasure*regret <= 0
            Constraint2 is bellmanflow
            """
            state_n = env.possible_states
            jact_n = env.joint_act_nums
            qt_table_vals = [qt_tables[i].qt for i in range(env.num_agents)]
            tran_m = np.array(env.transition_matrix)
            #start = time.time()

            """
            Set all constraints
            """
            all_constraints = []
            for i in range(env.num_agents):
                all_constraints.append({'type':'ineq','fun':regret_constraint,'args':(state_n,jact_n,qt_table_vals[i],ori_idx[i],alter_idx[i])})
            all_constraints.append({'type':'eq','fun':bellmanflow_constraint,'args':(state_n,jact_n,tran_m)})
            if algo_name == 'constrained':
                all_constraints.append({'type':'ineq','fun':risk_constraint,'args':(env,0.05)})
            all_constraints = tuple(all_constraints)

            """
            Set objective function
            """
            if obj_name =='DBCE':
                obj_func = cal_risk
                arg_used = env
            elif obj_name == 'MinRew':
                obj_func = cal_risk_reward
                arg_used = env
            elif obj_name == 'MinAbsRew':
                obj_func = cal_abs_rew
                arg_used = env
            elif obj_name == 'MaxRew':
                obj_func = cal_neg_total_reward
                arg_used = env
            elif obj_name == 'MaxNonRiskRew':
                obj_func = cal_neg_nonrisk_reward
                arg_used = env

            
            result_om = opt.minimize(
                fun=obj_func,
                args=arg_used,
                x0=np.repeat(gamma_v / (env.possible_states * env.joint_act_nums),
                             env.possible_states * env.joint_act_nums).reshape(
                                 env.possible_states, env.joint_act_nums),
                bounds=[
                    (0,None)
                    for _ in range(env.possible_states * env.joint_act_nums)
                ],
                options={
                    'disp': False,
                    'maxiter': 10000
                },
                constraints=all_constraints)

            #print("Time use %f" % (time.time() - start))
            om = result_om.x
            result = cal_risk(om, env)
            print("#####################    Risk now is %f     ##########################" % result)
            reg_vals = [-regret_constraint(om,state_n, jact_n, qt_table_vals[i], ori_idx[i], alter_idx[i]) for i in range(env.num_agents)]
            bf_val = bellmanflow_constraint(om,state_n,jact_n,tran_m)
            print("BF Constraint Violated: %f" % max(abs(bf_val)))
            max_reg_vals = [np.max(reg_vals[i]) for i in range(env.num_agents)]
            print("Regret Constraint Violated: %f" % (max(max_reg_vals)))
            policy.derive_pi(
                om.reshape(env.possible_states, env.joint_act_nums))
            exp_rew = cal_qval(om,qt_table_vals,env)
            writer.writerow(
                [obj_name,
                 algo_name,
                 str(epc),
                 str(iter),
                 str(float(result)),
                 str(float(exp_rew)),
                 str(float(max(abs(bf_val)))),
                 str(float(max(max_reg_vals)))])
            f.close()
            
if __name__ == "__main__":
    main('DBCE','Vanilla','SurviveSmall')
