3.17:
修改了main_Traffic中的参数，现在horizon来自于argument.py。
修改了argument中的horizon与max_iteration。初期试验结果表明traffic环境中收敛较快，现在iteration从100减少到了30.对偶上升的迭代次数增加到了50.
现在Traffic实验中使用的危险电量阈值由0.4提升到了0.6
3.17
修改了Modifyreward的危险reward
agent数量由4增加到了6
充电桩和target的数量减少了一半
在Traffic环境中，我们现在对constrained方法使用阈值0.1

3.20
修改了Traffic环境的充能，现在只会充能1点
修改了Traffic环境中的target数量与充电桩数量（target增加了，充电桩变少了）
修改了实验的epoch，从5到2
修改了学习率从1e-3到1e-4
现在游戏的horizon为200
mujocogather环境的危险判断指标更加严格了
为mujocogather环境增加了归一化，同时修改了cal_risk与cal_qval
修改了traffic环境的reward setting使reward数字变小
为traffic环境增加了终止条件（所有车辆都没电了）
为代码增加了梯度裁剪

3.21
对Traffic环境增加了连通性判断，现在该问题只在强连通分量内运行
删除了Traffic环境从本地文件读取transition的功能
修改了学习率从1e-4到3e-4
增加了对偶上升的内循环次数让算法更稳定
增加了对偶上升总次数
为充电行为增加了惩罚性reward

3.22
试用1e-3学习率
修改q-net的迭代次数
修改了grad_clip的参数为1
修改了神经网络的结构，尝试使用更复杂的神经网络
在不要求为正数输出的情况，激活函数尝试使用LeakyRelu
增加了记录训练过程的功能

3.30
修改了评估模型方法
修改了神经网络单元数
修改了Traffic的观测值。现在agent会观测到target和reload的信息
修改了神经网络初始化方法

4.1
修改了Traffic环境中由于改动观测值导致的严重bug

4.3
修改clipvalue
修改norm_density
增加一个不用神经网络的lambda和mu的版本